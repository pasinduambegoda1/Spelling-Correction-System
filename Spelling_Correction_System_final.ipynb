{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8b8f3d28-ea1c-4c35-9aa6-d8d4654e9e01",
      "metadata": {
        "id": "8b8f3d28-ea1c-4c35-9aa6-d8d4654e9e01"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this assignment, you will embark on the exciting task of constructing a spell checker utilizing the noisy channel model. Your mission involves the implementation of a segment dedicated to the noisy-channel model for spelling correction, complemented by the integration of diverse language models. During the testing phase, you will encounter sentences intentionally infused with a single typing error. The objective is to meticulously identify and rectify the error by selecting the correction that attains the highest likelihood under the noisy-channel model. Additionally, your language model will serve as the crucial prior in this correction process. The effectiveness of your spell checker will be assessed based on accuracy, calculated as the ratio of valid corrections to the total number of test sentences. Prepare to delve into the intricacies of language modeling for spell-checking.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e7dbb81-a456-4226-a36f-54b20494a723",
      "metadata": {
        "id": "5e7dbb81-a456-4226-a36f-54b20494a723"
      },
      "source": [
        "# Preliminaries\n",
        "\n",
        "In this assignment, you will be utilizing three essential class structures provided to you: `LexicalEntry`, `Sentence`, and `Corpus`.\n",
        "Before delving into the core tasks of the assignment, it is imperative to familiarize yourself with these foundational structures. Each class serves a distinct purpose in facilitating your understanding of lexical entries, sentence structures, and corpus organization.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qr0MzZcWKat",
        "outputId": "3931dc34-c7ab-4e65-f2d8-1b9e3ab161d4"
      },
      "id": "4qr0MzZcWKat",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/4.2D - Spelling Correction System/')"
      ],
      "metadata": {
        "id": "wYeMf_zCXZ11"
      },
      "id": "wYeMf_zCXZ11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyxdameraulevenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_BarorLxuqC",
        "outputId": "8a5db80c-1975-4375-d465-a769ef13ae7e"
      },
      "id": "j_BarorLxuqC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyxdameraulevenshtein in /usr/local/lib/python3.11/dist-packages (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd75d123-1fa4-4469-aba7-2e99fa491a22",
      "metadata": {
        "id": "dd75d123-1fa4-4469-aba7-2e99fa491a22"
      },
      "outputs": [],
      "source": [
        "from utils import LexicalEntry, Sentence, Corpus, SpellingResult, group_n_words\n",
        "import collections\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7f57137-981d-4fb6-9241-255b78bfbe92",
      "metadata": {
        "id": "a7f57137-981d-4fb6-9241-255b78bfbe92"
      },
      "source": [
        "## LexicalEntry\n",
        "\n",
        "The `LexicalEntry` class represents a unit of information about a word and its potential error within the context of a spelling correction system. Here's a summary of its key functionalities:\n",
        "\n",
        "- **Initialization:** The class is initialized with a correct `word` and an optional `error` word, which defaults to an empty string if not provided.\n",
        "- **Fixing Error:** The `fixError` method creates a new `LexicalEntry` object with the same correct word but an empty error attribute, essentially fixing the error.\n",
        "- **Error Checking:** The `hasError` method checks if the `LexicalEntry` object has an error, returning **True** if an error is present and **False** otherwise.\n",
        "- **Validity Testing:** The `isValidTest` method determines if the error in the `LexicalEntry` is within an edit distance of **one** and contains no numerics or punctuation. It returns **True** if the conditions are met, and **False** otherwise.\n",
        "- **String Representation:** The `__str__` method creates a string representation of the `LexicalEntry` object in the format **word (error)** if an error is present, and simply **word** if there is no error.\n",
        "\n",
        "This class provides a comprehensive set of methods to manage and analyze lexical entries, making it a crucial component in the broader spell-checking system. It encompasses functionality for error correction, error checking, validity testing, and string representation of lexical entries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05eab299-fdfd-43bf-b038-f18bd7a7058d",
      "metadata": {
        "id": "05eab299-fdfd-43bf-b038-f18bd7a7058d"
      },
      "source": [
        "### Usage examples\n",
        "\n",
        "Let's instantiate a `LexicalEntry` object with the correct word **love** and an associated error word **lov**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "452056ef-1faf-48f2-9bde-e14f7d16b24d",
      "metadata": {
        "id": "452056ef-1faf-48f2-9bde-e14f7d16b24d"
      },
      "outputs": [],
      "source": [
        "lexical_entry = LexicalEntry(\"love\", \"lov\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8089e60-0bac-4191-a929-86ba002476d9",
      "metadata": {
        "id": "e8089e60-0bac-4191-a929-86ba002476d9"
      },
      "source": [
        "Let's check if there is an error in that `LexicalEntry` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7faf42d-2ee5-4d17-a474-91fe0fa09d59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7faf42d-2ee5-4d17-a474-91fe0fa09d59",
        "outputId": "518237cf-4cd7-4e10-94f9-c483e77b7dd7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "lexical_entry.hasError()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1473888d-f40f-4a46-89ee-6d867f2b2d1c",
      "metadata": {
        "id": "1473888d-f40f-4a46-89ee-6d867f2b2d1c"
      },
      "source": [
        "Let's check if the error is withing Edit distance of 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72c4eba4-d373-4faa-af8b-6e6b449e8c69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72c4eba4-d373-4faa-af8b-6e6b449e8c69",
        "outputId": "93756fec-d420-438b-ce03-7ee340f56dee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "lexical_entry.isValidTest()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c473d98a-e402-470b-9409-a7d8178a7bc0",
      "metadata": {
        "id": "c473d98a-e402-470b-9409-a7d8178a7bc0"
      },
      "source": [
        "Let's now print it using the `__str__` method to show the error:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fe3179f-4e30-43f5-9c8d-dd91971f2409",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fe3179f-4e30-43f5-9c8d-dd91971f2409",
        "outputId": "af47a122-5ae5-466a-89f7-5640cfa8e7d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "love (lov)\n"
          ]
        }
      ],
      "source": [
        "print(lexical_entry)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b4f90bf-3634-4a31-be23-3bb07bab0e5c",
      "metadata": {
        "id": "8b4f90bf-3634-4a31-be23-3bb07bab0e5c"
      },
      "source": [
        "Finally, let's fix the error and print it again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fc4efd5-fdda-4071-bb6f-424c2e320bba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fc4efd5-fdda-4071-bb6f-424c2e320bba",
        "outputId": "b33b51b5-3a82-4715-dfe2-ed7bae6b23fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "love\n"
          ]
        }
      ],
      "source": [
        "lexical_entry_fixed = lexical_entry.fixError()\n",
        "print(lexical_entry_fixed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8f73fa4-9e87-4827-bae8-931fc25e20a1",
      "metadata": {
        "id": "f8f73fa4-9e87-4827-bae8-931fc25e20a1"
      },
      "source": [
        "## Sentence\n",
        "\n",
        "The `Sentence` class is designed to manage and manipulate a sequence of `LexicalEntry` instances, each representing a word with potential errors in a sentence. Here is a summary of its key methods:\n",
        "\n",
        "- **Initialization:** The class is initialized with a list of `LexicalEntries` representing the words in a sentence. The default is an empty list if not provided.\n",
        "- **Error and Correction Retrieval:** Methods such as `getErrorSentence` and `getCorrectSentence` return lists of strings containing the sentence with errors or corrections, respectively.\n",
        "- **Correction Verification:** The `isCorrection` method checks if a given list of strings is a correction of the sentence.\n",
        "- **Error Index Detection:** The `getErrorIndex` method returns the index of the first error in the sentence or -1 if there is no error.\n",
        "- **Index-based Access and Modification:** Methods like `get` retrieve the `LexicalEntry` at a specified index, while `update` modifies the entry at a given index.\n",
        "- **Sentence Cleaning:** The `cleanSentence` method creates a new sentence with all `LexicalEntry` instances having errors removed.\n",
        "- **Empty Check:** The `isEmpty` method checks if the sentence is empty.\n",
        "- **Appending and Length Calculation:** The `append` method adds a `LexicalEntry` to the sentence, and the `__len__` method returns the length of the sentence.\n",
        "- **String Representation:** The `__str__` method generates a string representation of the `Sentence` object, combining the string representations of individual `LexicalEntry` instances in the sentence.\n",
        "\n",
        "\n",
        "Overall, the `Sentence` class provides a robust set of methods for managing and analyzing sentences with potential spelling errors, making it a valuable component in the context of spelling correction systems.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "761a95b8-c34c-4f8f-b9ca-4fb5157d9aac",
      "metadata": {
        "id": "761a95b8-c34c-4f8f-b9ca-4fb5157d9aac"
      },
      "source": [
        "### Usage examples\n",
        "\n",
        "Let's instantiate a `Sentence` object with the sentence **\"i study at Deakin University\"** with a correct word **study** and an associated error word **stud**. ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7978924-c5ed-4a1b-b146-d1d2da1b9055",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7978924-c5ed-4a1b-b146-d1d2da1b9055",
        "outputId": "fa54ac25-8503-4e42-9e30-ad40d28fc668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i study (stud) at Deakin University\n"
          ]
        }
      ],
      "source": [
        "lst = [LexicalEntry(\"i\"), LexicalEntry(\"study\", \"stud\"), LexicalEntry(\"at\"), LexicalEntry(\"Deakin\"), LexicalEntry(\"University\")]\n",
        "my_sentence = Sentence(lst)\n",
        "# Print the sentence\n",
        "print(my_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b388f26-ac35-47eb-af27-7ab442718e07",
      "metadata": {
        "id": "8b388f26-ac35-47eb-af27-7ab442718e07"
      },
      "source": [
        "Let's first print the error sentence and the correct sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8fa7101-cb9e-443c-aae0-8203654c262c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8fa7101-cb9e-443c-aae0-8203654c262c",
        "outputId": "25069168-c82b-42c9-e808-43735fd97ade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The incorrect sentence is: ['i', 'stud', 'at', 'Deakin', 'University']\n",
            "The correct sentence is: ['i', 'study', 'at', 'Deakin', 'University']\n"
          ]
        }
      ],
      "source": [
        "print(f'The incorrect sentence is: {my_sentence.get_error_sentence()}')\n",
        "print(f'The correct sentence is: {my_sentence.get_correct_sentence()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae2b0481-e071-4f97-9526-7056e6682eb8",
      "metadata": {
        "id": "ae2b0481-e071-4f97-9526-7056e6682eb8"
      },
      "source": [
        "The `getErrorIndex` method can be used to return the index of the first error in the sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6f3b927-f910-4f9c-ba6a-3eac8e254262",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6f3b927-f910-4f9c-ba6a-3eac8e254262",
        "outputId": "4f7bb296-f507-4dc6-9ed6-321f802ff997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The correct sentence is: 1\n"
          ]
        }
      ],
      "source": [
        "print(f'The correct sentence is: {my_sentence.get_error_index()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e489521-78e1-4315-941e-f9038be4bb95",
      "metadata": {
        "id": "4e489521-78e1-4315-941e-f9038be4bb95"
      },
      "source": [
        "We can check if another sentence is a correction to our sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f606e810-c895-43db-aa5e-c88f0c15ffa2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f606e810-c895-43db-aa5e-c88f0c15ffa2",
        "outputId": "f623b9c5-9804-4f0f-c28e-73569603cd80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "print(my_sentence.is_correction([\"i\", \"study\", \"at\", \"Deakin\", \"University\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "509172be-bb00-4391-b90f-6d9899e207cd",
      "metadata": {
        "id": "509172be-bb00-4391-b90f-6d9899e207cd"
      },
      "source": [
        "You can use the `get` method to retrieve the `LexicalEntry` at a specified index and update it if needed using `update`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2e1edaf-22d4-4291-b14c-f9dc8f04d826",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2e1edaf-22d4-4291-b14c-f9dc8f04d826",
        "outputId": "63f1eac7-d178-41dc-8468-d267866803a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i\n",
            "I study (stud) at Deakin University\n"
          ]
        }
      ],
      "source": [
        "# Print LexicalEntry at index 0\n",
        "print(my_sentence.get(0))\n",
        "# Update it with I and print sentence\n",
        "my_sentence.update(0, LexicalEntry('I'))\n",
        "print(my_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bf4ed92-d2fc-40e6-afa5-712004398003",
      "metadata": {
        "id": "7bf4ed92-d2fc-40e6-afa5-712004398003"
      },
      "source": [
        "Finally, let's clean out sentence from errors and return a clean sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f998fa69-e8a1-4223-ae07-31a5f83dc4a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f998fa69-e8a1-4223-ae07-31a5f83dc4a6",
        "outputId": "35e7d151-73c1-4a9f-c1df-d4d77fa2af28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I study at Deakin University\n"
          ]
        }
      ],
      "source": [
        "my_fixed_sentence = my_sentence.clean_sentence()\n",
        "print(my_fixed_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99276e5e-18e0-4986-ac2d-97799e400f98",
      "metadata": {
        "id": "99276e5e-18e0-4986-ac2d-97799e400f98"
      },
      "source": [
        "## Corpus\n",
        "\n",
        "The `Corpus` class represents a collection of sentences from a dataset and provides methods for processing and generating test cases with eligible spelling errors. Here's a breakdown of its key methods:\n",
        "\n",
        "- **Initialization:** The class is initialized with an optional filename pointing to the Holbrook dataset. If a filename is provided, the dataset is read and processed; otherwise, an empty corpus is created.\n",
        "- **Reading and Processing Data:** The `read_corpus` method reads data from a file, processes it into a list of `Sentence` objects, and populates the corpus.\n",
        "- **Processing Line:** The `processLine` method takes a line from the dataset, removes punctuation, converts to lowercase, and creates a `Sentence` object with `LexicalEntry` instances.\n",
        "- **Generating Test Cases:** The `generateTestCases` method creates a list of sentences with exactly one eligible spelling error by iterating through the corpus and selecting appropriate words for testing.\n",
        "- **Vocabulary Retrieval:** The `get_vocabulary` method retrieves the vocabulary from the corpus, consisting of unique words.\n",
        "\n",
        "In summary, the Corpus class serves as a container for sentences, offering methods for data initialization, processing, test case generation, and vocabulary retrieval. It plays a crucial role in facilitating the handling and manipulation of linguistic data within the context of spelling correction and language modeling.\n",
        "\n",
        "Let's first create our `Corpus` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebf8c553-88b1-4df8-bb18-a3abe5902bb9",
      "metadata": {
        "id": "ebf8c553-88b1-4df8-bb18-a3abe5902bb9"
      },
      "outputs": [],
      "source": [
        "my_corpus = Corpus('/content/drive/MyDrive/4.2D - Spelling Correction System/data/trainset.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74e19f67-6f2e-4434-9162-50721ed0caad",
      "metadata": {
        "id": "74e19f67-6f2e-4434-9162-50721ed0caad"
      },
      "source": [
        "We now print a few sentences from it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d768ffa2-7238-4e57-8cec-aa9e84410b1e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d768ffa2-7238-4e57-8cec-aa9e84410b1e",
        "outputId": "1c056547-e010-49ac-b0af-6a5243e1b824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> 1 </s>\n",
            "<s> nigel thrush page 48 </s>\n",
            "<s> i have four in my family dad mum and sister (siter) </s>\n",
            "<s> my dad works at melton </s>\n",
            "<s> my sister (siter) goes (go) to tonbury </s>\n",
            "<s> my mum goes out sometimes </s>\n",
            "<s> i go to bridgebrook i go out sometimes on tuesday night i go to youth club (clob) </s>\n",
            "<s> on thursday nights i go bellringing on saturdays i go down to the farm </s>\n",
            "<s> on sundays i go to church </s>\n",
            "<s> i go to bed at 10 o clock i watch (wakh) tv at 5 o clock i live in a house </s>\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    print(my_corpus.corpus[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b91f6d5f-6349-4349-9dfb-055fa229e14b",
      "metadata": {
        "id": "b91f6d5f-6349-4349-9dfb-055fa229e14b"
      },
      "source": [
        "# The Noisy Channel Spell Correction Model\n",
        "\n",
        "The foundation of the noisy channel model lies in treating a misspelled word as if it underwent distortion while being transmitted through a noisy communication channel. The objective is to construct a model of this noisy channel and determine the true word by evaluating the likelihood of each candidate word given the observed misspelling. The Bayesian inference approach is employed, seeking the word $w$ that maximizes the conditional probability $P(w|x)$. Mathematically expressed as:\n",
        "\n",
        "$$\\hat{w} = \\underset{w\\in V}{\\operatorname{argmax}} P(w|x) $$\n",
        "\n",
        "\n",
        "this signifies choosing the word with the highest likelihood from the vocabulary $V$. Bayesian classification leverages Bayes' rule to transform this into $P(x|w)P(w) / P(x)$, and by dropping the denominator $P(x)$, we get  the following simplified formula:\n",
        "\n",
        "$$\\hat{w} = \\underset{w\\in V}{\\operatorname{argmax}}  \\overbrace{P(x|w)}^\\text{Channel Model} \\times  \\underbrace{P(w)}_\\text{Language Model} $$\n",
        "\n",
        "In essence, the model computes the most probable word given an observed misspelling by multiplying the prior $P(w)$ (**Language Model**) and the likelihood $P(x|w)$ (**the Channel Model**). The noisy channel algorithm, is then applied to correct non-word spelling errors by ranking candidate corrections according to the previous Equation and selecting the highest-ranked one. This process involves evaluating the likelihood (**Channel Model**) $P(x|w)$ and the prior (**Language Model**) $P(w)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98cafa39-b3b3-44fe-8093-8dcdfde32ea8",
      "metadata": {
        "id": "98cafa39-b3b3-44fe-8093-8dcdfde32ea8"
      },
      "source": [
        "## Channel Model\n",
        "\n",
        "The estimation of the likelihood  $P(x|w)$, referred to as the channel model or error model, is a crucial aspect of the noisy channel model. This model aims to capture the probability that a word  $w$ w will be mistyped, taking into account various factors. While a perfect model might consider factors like the typist's identity or handedness, a practical estimate can be obtained by examining the **Minimal Damerau-Levenshtein Edit Distance** between two strings, where edits are:\n",
        "\n",
        "- Insertion\n",
        "- Deletion\n",
        "- Substitution\n",
        "- Transposition of two adjacent letters\n",
        "\n",
        "For instance, letters like **'m'** and **'n'** are often substituted due to both their phonetic similarity and their adjacency on the keyboard. A simple model might estimate  $P(acressâˆ£across)$ by analyzing the frequency of the letter **'e'** being substituted for **'o'** in a large corpus of errors. To compute these probabilities systematically, a confusion matrix is employed, which lists the number of times one element was confused with another. Specifically, the file `data/count_1edit.txt` which provides counts for all single-edit spelling correction edits. For example, the line `da|d 13` suggests that the correction of **'d'** to **'da'** has been observed 13 times in the data, whereas `e|i 917` indicates that the substitution of **'e'** to **'i'** has been observed 917 times in the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f5c7b48-a121-49a2-93e3-daf290acb462",
      "metadata": {
        "id": "8f5c7b48-a121-49a2-93e3-daf290acb462"
      },
      "source": [
        "### <span style=\"color:red\"><b>Task 1</b></span>\n",
        "\n",
        "Let's build our Channel Model and by building the model to calculate our likelihood $P(x|w)$:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelModel:\n",
        "    alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "\n",
        "    def __init__(self, edit_file, vocabulary):\n",
        "        \"\"\"\n",
        "        Initializes the ChannelModel with an edit file and a vocabulary set.\n",
        "        \"\"\"\n",
        "        self.edit_file = edit_file\n",
        "        self.edit_table = self._read_edit_table(self.edit_file)\n",
        "        self.vocabulary = vocabulary\n",
        "\n",
        "    def compute_edit_probabilities(self, word):\n",
        "        \"\"\"\n",
        "        Computes p(x|word) edit model for all valid word x **that are in the vocabulary**.\n",
        "        Returns a dictionary mapping x -> p(x|word).\n",
        "        \"\"\"\n",
        "        counts = collections.defaultdict(int)\n",
        "\n",
        "        # Generate and count edits\n",
        "        for edit_type in ['deletions', 'insertions', 'transpositions', 'replacements']:\n",
        "            method = getattr(self, f'_process_{edit_type}')\n",
        "            for candidate, count in method(word).items():\n",
        "                counts[candidate] += count\n",
        "\n",
        "        # Normalize counts\n",
        "        total = sum(counts.values())\n",
        "        selfCount = max(9 * total, 1)\n",
        "        counts[word] = selfCount\n",
        "        total += selfCount\n",
        "\n",
        "        probs = {candidate: count / total for candidate, count in counts.items()}\n",
        "\n",
        "        return probs\n",
        "\n",
        "    def _process_deletions(self, word):\n",
        "        counts = {}\n",
        "        for i in range(len(word)):\n",
        "            deleted = word[:i] + word[i+1:]\n",
        "            if deleted in self.vocabulary:\n",
        "                s1 = (word[i - 1] if i > 0 else '') + word[i]\n",
        "                s2 = (word[i - 1] if i > 0 else '')\n",
        "                count = self._edit_count(s1, s2)\n",
        "                if count > 0:\n",
        "                    counts[deleted] = count\n",
        "        return counts\n",
        "\n",
        "    def _process_insertions(self, word):\n",
        "        counts = {}\n",
        "        for i in range(len(word) + 1):\n",
        "            for c in self.alphabet:\n",
        "                inserted = word[:i] + c + word[i:]\n",
        "                if inserted in self.vocabulary:\n",
        "                    prev_char = word[i - 1] if i > 0 else ''\n",
        "                    s1 = prev_char\n",
        "                    s2 = prev_char + c\n",
        "                    count = self._edit_count(s1, s2)\n",
        "                    if count > 0:\n",
        "                        counts[inserted] = count\n",
        "        return counts\n",
        "\n",
        "    def _process_transpositions(self, word):\n",
        "        counts = {}\n",
        "        for i in range(len(word) - 1):\n",
        "            if word[i] != word[i+1]:\n",
        "                transposed = word[:i] + word[i+1] + word[i] + word[i+2:]\n",
        "                if transposed in self.vocabulary:\n",
        "                    s1 = word[i] + word[i+1]\n",
        "                    s2 = word[i+1] + word[i]\n",
        "                    count = self._edit_count(s1, s2)\n",
        "                    if count > 0:\n",
        "                        counts[transposed] = count\n",
        "        return counts\n",
        "\n",
        "    def _process_replacements(self, word):\n",
        "        counts = {}\n",
        "        for i in range(len(word)):\n",
        "            original = word[i]\n",
        "            for c in self.alphabet:\n",
        "                if c != original:\n",
        "                    replaced = word[:i] + c + word[i+1:]\n",
        "                    if replaced in self.vocabulary:\n",
        "                        count = self._edit_count(original, c)\n",
        "                        if count > 0:\n",
        "                            counts[replaced] = count\n",
        "        return counts\n",
        "\n",
        "\n",
        "    def _read_edit_table(self, file_name):\n",
        "        edit_table = {}\n",
        "        with open(file_name, encoding='latin-1') as f:\n",
        "            for line in f:\n",
        "                contents = line.strip().split(\"\\t\")\n",
        "                if len(contents) == 2:\n",
        "                    edit_table[contents[0]] = int(contents[1])\n",
        "        return edit_table\n",
        "\n",
        "    def _edit_count(self, s1, s2):\n",
        "        return self.edit_table.get(f\"{s1}|{s2}\", 0)\n",
        "\n",
        "    def edit_distance(self, word1, word2):\n",
        "        \"\"\"\n",
        "        Computes the Damerau-Levenshtein edit distance between two words.\n",
        "        This is a basic implementation and can be optimized.\n",
        "        \"\"\"\n",
        "        d = {}\n",
        "        len1 = len(word1)\n",
        "        len2 = len(word2)\n",
        "        for i in range(-1, len1 + 1):\n",
        "            d[(i, -1)] = i + 1\n",
        "        for j in range(-1, len2 + 1):\n",
        "            d[(-1, j)] = j + 1\n",
        "\n",
        "        for i in range(len1):\n",
        "            for j in range(len2):\n",
        "                cost = 0 if word1[i] == word2[j] else 1\n",
        "                d[(i, j)] = min(\n",
        "                    d[(i - 1, j)] + 1,  # deletion\n",
        "                    d[(i, j - 1)] + 1,  # insertion\n",
        "                    d[(i - 1, j - 1)] + cost,  # substitution\n",
        "                )\n",
        "                if i > 0 and j > 0 and word1[i] == word2[j - 1] and word1[i - 1] == word2[j]:\n",
        "                    d[(i, j)] = min(d[(i, j)], d[(i - 2, j - 2)] + cost)  # transposition\n",
        "\n",
        "        return d[(len1 - 1, len2 - 1)]"
      ],
      "metadata": {
        "id": "t3i6cuZD4Orp"
      },
      "id": "t3i6cuZD4Orp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "19e54a26-63a0-4a68-a3ca-a40de552a2b4",
      "metadata": {
        "id": "19e54a26-63a0-4a68-a3ca-a40de552a2b4"
      },
      "source": [
        "Test you code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aab882b-672b-49a8-91df-e92ce716ec5e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aab882b-672b-49a8-91df-e92ce716ec5e",
        "outputId": "de50bb60-f567-4d53-9d83-bc893ed2e800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful\n",
            "Successful\n",
            "Successful\n",
            "Successful\n",
            "Successful\n",
            "Successful\n",
            "Successful\n",
            "Successful\n",
            "Successful\n",
            "Successful\n",
            "Successful\n",
            "Successful\n",
            "Successful\n"
          ]
        }
      ],
      "source": [
        "c = Corpus('/content/drive/MyDrive/4.2D - Spelling Correction System/data/trainset.txt')\n",
        "model = ChannelModel(edit_file=\"/content/drive/MyDrive/4.2D - Spelling Correction System/data/count_1edit.txt\", vocabulary=c.get_vocabulary())\n",
        "\n",
        "word = 'read'\n",
        "assert model.process_deletions(word) == {'red': 285}, 'Test failed'\n",
        "print('Successful')\n",
        "assert model.process_insertions(word) == {'ready': 8}, 'Test failed'\n",
        "print('Successful')\n",
        "assert model.process_replacements(word) == {'dead': 15, 'head': 4, 'lead': 45, 'road': 295, 'real': 4}, 'Test failed'\n",
        "print('Successful')\n",
        "\n",
        "word = 'there'\n",
        "assert model.process_insertions(word) == {'theyre': 36, 'theres': 258}, 'Test failed'\n",
        "print('Successful')\n",
        "assert model.process_transpositions(word) == {'three': 189}, 'Test failed'\n",
        "print('Successful')\n",
        "assert model.process_replacements(word) == {'where': 3, 'these': 14}, 'Test failed'\n",
        "print('Successful')\n",
        "\n",
        "word = 'more'\n",
        "assert model.process_insertions(word) == {'moore': 24}, 'Test failed'\n",
        "print('Successful')\n",
        "assert model.process_replacements(word) == {'fore': 2, 'sore': 3, 'move': 2}, 'Test failed'\n",
        "print('Successful')\n",
        "\n",
        "word = 'hello'\n",
        "assert model.process_deletions(word) == {'hell': 18}, 'Test failed'\n",
        "print('Successful')\n",
        "\n",
        "word = 'dad'\n",
        "assert model.process_insertions(word) == {'dead': 85, 'dads': 15}, 'Test failed'\n",
        "print('Successful')\n",
        "assert model.process_replacements(word) == {'bad': 37, 'had': 1, 'did': 559, 'day': 2}, 'Test failed'\n",
        "print('Successful')\n",
        "\n",
        "assert model.compute_edit_probabilities('read') == {'red': 0.04344512195121951, 'ready': 0.0012195121951219512,\n",
        "                                                    'dead': 0.0022865853658536584, 'head': 0.0006097560975609756,\n",
        "                                                    'lead': 0.006859756097560976, 'road': 0.04496951219512195,\n",
        "                                                    'real': 0.0006097560975609756, 'read': 0.9}, 'Test failed'\n",
        "print('Successful')\n",
        "\n",
        "assert model.compute_edit_probabilities('reda') == {'red': 0.1, 'reda': 0.9}, 'Test failed'\n",
        "print('Successful')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e257c416-086a-48ed-8212-e68684a24607",
      "metadata": {
        "id": "e257c416-086a-48ed-8212-e68684a24607"
      },
      "source": [
        "## Language Models\n",
        "\n",
        "Language models are a fundamental component of natural language processing and artificial intelligence, playing a crucial role in understanding and generating human-like text. These models are designed to comprehend the intricate patterns, structures, and semantics inherent in language, enabling machines to interact with and generate coherent and contextually relevant text.\n",
        "\n",
        "At their core, language models are statistical and machine learning-based systems that learn the inherent rules and patterns of a language from vast amounts of textual data. Their primary objective is to predict the next word or sequence of words in a given context, harnessing the power of probabilistic relationships within a language. The ability to predict the likelihood of various word combinations empowers these models to capture syntactic, semantic, and contextual nuances, making them versatile tools for a wide array of applications.\n",
        "\n",
        "Here, your task is to implement three Language Models.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2022e31c-e266-4fb2-b911-b7c53b0127f0",
      "metadata": {
        "id": "2022e31c-e266-4fb2-b911-b7c53b0127f0"
      },
      "source": [
        "### Uniform Language Model\n",
        "\n",
        "\n",
        "Here your task is to build a simple uniform language model, which is capable of training on a given corpus and calculating the log-probability of sentences based on a uniform distribution of words.\n",
        "Mathematically, for a sentence $s=w_1,w_2,w_3,\\dots,w_n$ , the uniform language model score is calculated as follows:\n",
        "$$\\text{Uniform LM Score}(s) = log(P(s))=\\sum_{w \\in s } log(\\frac{1}{|V|})$$\n",
        "where $|V|$ is the size of the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LaplaceBigramLanguageModel:\n",
        "    def __init__(self, corpus):\n",
        "        \"\"\"\n",
        "        Initialize your data structures in the constructor.\n",
        "        Parameters:- corpus: The corpus to train the language model.\n",
        "        \"\"\"\n",
        "        self.unigram_counts = {}  # Changed variable name\n",
        "        self.bigram_counts = {}   # Changed variable name\n",
        "        self.vocabulary = set()\n",
        "        self.total_words = 0      # Changed variable name\n",
        "        self.train(corpus)\n",
        "\n",
        "    def train(self, corpus):\n",
        "        \"\"\"\n",
        "        Takes a corpus and trains your language model.\n",
        "        Compute any counts or other corpus statistics in this function.\n",
        "        Parameters:- corpus: The corpus to train the language model.\n",
        "        \"\"\"\n",
        "        for sentence in corpus.corpus:\n",
        "            prev_word = None\n",
        "            for entry in sentence.data:\n",
        "                word = entry.word\n",
        "                # Unigram count\n",
        "                self.unigram_counts[word] = self.unigram_counts.get(word, 0) + 1\n",
        "                self.total_words += 1\n",
        "                self.vocabulary.add(word)\n",
        "                # Bigram count\n",
        "                if prev_word is not None:\n",
        "                    bigram = (prev_word, word)\n",
        "                    self.bigram_counts[bigram] = self.bigram_counts.get(bigram, 0) + 1\n",
        "                prev_word = word\n",
        "\n",
        "    def score(self, sentence):\n",
        "        \"\"\"\n",
        "        Takes a list of strings as an argument and returns the log-probability of the\n",
        "        sentence using your language model. Use whatever data you computed in train() here.\n",
        "        Parameters:- sentence (list): A list of strings representing the input sentence.\n",
        "        Returns:- float: The log-probability of the sentence.\n",
        "        \"\"\"\n",
        "        score = 0.0\n",
        "        vocab_size = len(self.vocabulary)\n",
        "        for i in range(1, len(sentence)):\n",
        "            prev = sentence[i- 1]\n",
        "            curr = sentence[i]\n",
        "            bigram = (prev, curr)\n",
        "            bigram_count = self.bigram_counts.get(bigram, 0)\n",
        "            unigram_count = self.unigram_counts.get(prev, 0)\n",
        "            # Add a small epsilon to avoid log(0) if unigram_count is 0\n",
        "            prob = (bigram_count + 1) / (unigram_count + vocab_size)\n",
        "            score += math.log(prob)\n",
        "        return score"
      ],
      "metadata": {
        "id": "z2JiUE0g5Cgr"
      },
      "id": "z2JiUE0g5Cgr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "aa193169-962a-4b4e-9a06-9e2d4bb9f2dc",
      "metadata": {
        "id": "aa193169-962a-4b4e-9a06-9e2d4bb9f2dc"
      },
      "source": [
        "Test your implementation here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8b7b107-b67a-43ed-912f-9a353404a212",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8b7b107-b67a-43ed-912f-9a353404a212",
        "outputId": "484bbbd0-3f6f-4cf1-9dd3-5ed1170cd580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seccessful\n",
            "Seccessful\n",
            "Seccessful\n",
            "Seccessful\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Trains all of the language models and tests them on the dev data.\"\"\"\n",
        "trainPath = '/content/drive/MyDrive/4.2D - Spelling Correction System/data/trainset.txt'\n",
        "trainingCorpus = Corpus(trainPath)\n",
        "\n",
        "uniformLM = UniformLanguageModel(trainingCorpus)\n",
        "assert uniformLM.score(['I','am', 'Australian']) == -22.245525328839886, 'Test failed'\n",
        "print('Seccessful')\n",
        "assert uniformLM.score(['I','love','Deakin','University']) == -29.66070043845318, 'Test failed'\n",
        "print('Seccessful')\n",
        "assert uniformLM.score(['I','stud', 'CS','in','Australia']) == -37.07587554806648, 'Test failed'\n",
        "print('Seccessful')\n",
        "assert uniformLM.score(['I','am', 'enrolled', 'in', 'SIT770', 'as', 'a', 'graduate','student']) == -66.73657598651965, 'Test failed'\n",
        "print('Seccessful')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1521de28-3b4d-4c40-8ef1-512e19f7803d",
      "metadata": {
        "id": "1521de28-3b4d-4c40-8ef1-512e19f7803d"
      },
      "source": [
        "### Laplace Unigram Language Model\n",
        "\n",
        "Your task here is to implement a unigram language model using Laplace (add-one) smoothing.  The score calculated should be computed by summing the logarithms of Laplace-smoothed counts for each word in the sentence and adjusting for the total word count.\n",
        "Mathematically, for a sentence $s=w_1,w_2,w_3,\\dots,w_n$ , the Laplace Unigram Language Model score is calculated as follows:\n",
        "$$\\text{Laplace Unigram LM Score}(s)=log(P(s))=\\sum_{w_i \\in s } log(P(w_i))$$\n",
        "\n",
        "where the **Laplace-smoothed unigram probability** $P(w_i)$ is calculated as:\n",
        "\n",
        "$$P(w_i)=\\frac{c(w_i)+1}{\\sum_{ w_j\\in V } c(w_j) +|V|}$$\n",
        "\n",
        "where $|V|$ is the size of the vocabulary and $c(w_i)$ is the count indicating the number of times $w_i$ appears in the training set.\n",
        "The Laplace smoothing involves adding 1 to both the count of $c(w_i)$  and the denominator (total count of all words plus the vocabulary size). This ensures that even words not observed in the training set have a non-zero probability."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LaplaceUnigramLanguageModel:\n",
        "\n",
        "    def __init__(self, corpus):\n",
        "        \"\"\"Initialize your data structures in the constructor.\"\"\"\n",
        "        self.total = 0\n",
        "        self.vocabulary = set()\n",
        "        self.unigram_counts = {}  # Changed variable name to be less specific about smoothing\n",
        "        self.train(corpus)\n",
        "\n",
        "    def train(self, corpus):\n",
        "        \"\"\" Takes a corpus and trains your language model.\n",
        "            Compute any counts or other corpus statistics in this function.\n",
        "        \"\"\"\n",
        "        for sentence in corpus.corpus:\n",
        "            for lexical_entry in sentence.data:\n",
        "                word = lexical_entry.word\n",
        "                self.unigram_counts[word] = self.unigram_counts.get(word, 0) + 1\n",
        "                self.total += 1\n",
        "                self.vocabulary.add(word)\n",
        "\n",
        "    def score(self, sentence):\n",
        "        \"\"\" Takes a list of strings as argument and returns the log-probability of the\n",
        "            sentence using your language model. Use whatever data you computed in train() here.\n",
        "        \"\"\"\n",
        "        score = 0.0\n",
        "        V = len(self.vocabulary)\n",
        "        for word in sentence:\n",
        "            count = self.unigram_counts.get(word, 0)\n",
        "            # Apply Laplace smoothing: (count + 1) / (total + V)\n",
        "            smoothed_prob = (count + 1) / (self.total + V)\n",
        "            score += math.log(smoothed_prob)\n",
        "        return score\n",
        "\n"
      ],
      "metadata": {
        "id": "ta1w2wLR431v"
      },
      "id": "ta1w2wLR431v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b96b8490-b67d-4ef7-acc0-d8cecead87e8",
      "metadata": {
        "id": "b96b8490-b67d-4ef7-acc0-d8cecead87e8"
      },
      "source": [
        "Test your implementation here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6e926ab-a281-4183-ac80-4b0f34b95c6b",
      "metadata": {
        "id": "d6e926ab-a281-4183-ac80-4b0f34b95c6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea418335-0e17-46c9-ecd2-509c6bc34b53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seccessful\n",
            "Seccessful\n",
            "Seccessful\n",
            "Seccessful\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Trains all of the language models and tests them on the dev data.\"\"\"\n",
        "trainPath = '/content/drive/MyDrive/4.2D - Spelling Correction System/data/trainset.txt'\n",
        "trainingCorpus = Corpus(trainPath)\n",
        "\n",
        "laplaceUnigramLM = LaplaceUnigramLanguageModel(trainingCorpus)\n",
        "assert laplaceUnigramLM.score(['I','am', 'Australian']) == -25.14565287682459, 'Test failed'\n",
        "print('Seccessful')\n",
        "assert laplaceUnigramLM.score(['I','love','Deakin','University']) == -35.57756036152766, 'Test failed'\n",
        "print('Seccessful')\n",
        "assert laplaceUnigramLM.score(['I','stud', 'CS','in','Australia']) == -42.56080392732966, 'Test failed'\n",
        "print('Seccessful')\n",
        "assert laplaceUnigramLM.score(['I','am', 'enrolled', 'in', 'SIT770', 'as', 'a', 'graduate','student']) == -68.26327621084289, 'Test failed'\n",
        "print('Seccessful')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f762a69-aace-4593-9e57-60b966004f8a",
      "metadata": {
        "id": "3f762a69-aace-4593-9e57-60b966004f8a"
      },
      "source": [
        "### Laplace Bigram Language Model\n",
        "\n",
        "\n",
        "This LaplaceBigramLanguageModel class is designed to implement a bigram language model using Laplace (add-one) smoothing.  The train method processes the corpus, updating the bigram counts with Laplace smoothing. Similarly, the score method should calculate the log-probability of a given sentence based on the trained bigram language model, incorporating Laplace smoothing to handle unseen bigrams and unigrams.\n",
        "Mathematically, for a sentence $s=w_1,w_2,w_3,\\dots,w_n$ , the Laplace Bigram Language Model score is calculated as follows:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "\\text{Laplace Bigram Language Model}(s) &=log(\\prod_{w_i \\in s } P(w_i))\\\\\n",
        "&= log[P(w_1)P(w_2|w_1)P(w_3|w_2)\\cdots P(w_n|w_{n-1})]\\\\\n",
        "&= log(P(w_1))+log(P(w_2|w_1))+log(P(w_3|w_2))\\cdots +log(P(w_n|w_{n-1}))]\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "where the **Laplace-smoothed conditional probability** $P(w_i|w_{i-1})$ is calculated as:\n",
        "\n",
        "$$\n",
        "P(w_i|w_{i-1})=\\frac{c(w_i,w_{i-1})+1}{ c(w_{i-1}) +|V|}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "where $|V|$ is the size of the vocabulary, and $c(w_i)$  and $c(w_i,w_{i-1})$ are respectively unigram and bigrams in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNw_kBxnYpC5"
      },
      "outputs": [],
      "source": [
        "class LaplaceBigramLanguageModel:\n",
        "    def __init__(self, corpus):\n",
        "        \"\"\"\n",
        "        Initialize your data structures in the constructor.\n",
        "        Parameters:- corpus: The corpus to train the language model.\n",
        "        \"\"\"\n",
        "        self.LaplaceUnigramCounts = {}\n",
        "        self.LaplaceBigramCounts = {}\n",
        "        self.vocabulary = set()\n",
        "        self.total = 0\n",
        "        self.train(corpus)\n",
        "\n",
        "    def train(self, corpus):\n",
        "        \"\"\"\n",
        "        Takes a corpus and trains your language model.\n",
        "        Compute any counts or other corpus statistics in this function.\n",
        "        Parameters:- corpus: The corpus to train the language model.\n",
        "        \"\"\"\n",
        "        ## START YOUR CODE HERE\n",
        "        # Count unigram frequencies and build the vocabulary.\n",
        "        for sentence in corpus.corpus:\n",
        "            prev_word = None\n",
        "            for entry in sentence.data:\n",
        "                word = entry.word\n",
        "                # Unigram count\n",
        "                self.LaplaceUnigramCounts[word] = self.LaplaceUnigramCounts.get(word, 0) + 1\n",
        "                self.total += 1\n",
        "                self.vocabulary.add(word)\n",
        "                # Bigram count\n",
        "                if prev_word is not None:\n",
        "                    bigram = (prev_word, word)\n",
        "                    self.LaplaceBigramCounts[bigram] = self.LaplaceBigramCounts.get(bigram, 0) + 1\n",
        "                prev_word = word\n",
        "        ## END\n",
        "\n",
        "    def score(self, sentence):\n",
        "        \"\"\"\n",
        "        Takes a list of strings as an argument and returns the log-probability of the\n",
        "        sentence using your language model. Use whatever data you computed in train() here.\n",
        "        Parameters:- sentence (list): A list of strings representing the input sentence.\n",
        "        Returns:- float: The log-probability of the sentence.\n",
        "        \"\"\"\n",
        "        score = 0.0\n",
        "        ## START YOUR CODE HERE\n",
        "        vocab_size = len(self.vocabulary)\n",
        "        for i in range(1, len(sentence)):\n",
        "            prev = sentence[i- 1]\n",
        "            curr = sentence[i]\n",
        "            bigram = (prev, curr)\n",
        "            bigram_count = self.LaplaceBigramCounts.get(bigram, 0)\n",
        "            unigram_count = self.LaplaceUnigramCounts.get(prev, 0)\n",
        "            # Add a small epsilon to avoid log(0) if unigram_count is 0\n",
        "            prob = (bigram_count + 1) / (unigram_count + vocab_size)\n",
        "            score += math.log(prob)\n",
        "        ## END\n",
        "        return score"
      ],
      "id": "pNw_kBxnYpC5"
    },
    {
      "cell_type": "markdown",
      "id": "76ce044c-bf5d-4b70-b31a-e6be6f8b0358",
      "metadata": {
        "id": "76ce044c-bf5d-4b70-b31a-e6be6f8b0358"
      },
      "source": [
        "Test your implementation here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34db8ca8-22fa-49d8-bace-864c0a52344a",
      "metadata": {
        "id": "34db8ca8-22fa-49d8-bace-864c0a52344a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48970641-f7f7-44ea-d576-ed069afaf1cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seccessful\n",
            "Seccessful\n",
            "Seccessful\n",
            "Seccessful\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Trains all of the language models and tests them on the dev data.\"\"\"\n",
        "# Corrected trainPath to match the path used in the successful unigram test\n",
        "trainPath = '/content/drive/MyDrive/4.2D - Spelling Correction System/data/trainset.txt'\n",
        "trainingCorpus = Corpus(trainPath)\n",
        "\n",
        "laplaceBigramLM = LaplaceBigramLanguageModel(trainingCorpus)\n",
        "# Adding a tolerance for floating-point comparison\n",
        "assert abs(laplaceBigramLM.score(['I','am', 'Australian']) - (-14.847658917530413)) < 1e-9, 'Test failed'\n",
        "print('Seccessful')\n",
        "assert abs(laplaceBigramLM.score(['I','love','Deakin','University']) - (-22.252126012871237)) < 1e-9, 'Test failed'\n",
        "print('Seccessful')\n",
        "assert abs(laplaceBigramLM.score(['I','stud', 'CS','in','Australia']) - (-29.747159786723298)) < 1e-9, 'Test failed'\n",
        "print('Seccessful')\n",
        "assert abs(laplaceBigramLM.score(['I','am', 'enrolled', 'in', 'SIT770', 'as', 'a', 'graduate','student']) - (-58.90693709044222)) < 1e-9, 'Test failed'\n",
        "print('Seccessful')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d4812f1-7d38-4f2f-a7e5-e2c99a259c19",
      "metadata": {
        "id": "4d4812f1-7d38-4f2f-a7e5-e2c99a259c19"
      },
      "source": [
        "## Spell Correction Channel using Noisy Channel Model\n",
        "\n",
        "Now is time to consolidate all the components and assemble your SpellCorrect model, which comprises both the **Channel model** and the **Language Model**. The Channel model plays a pivotal role in estimating the likelihood of various corrections for observed misspelled words, while the Language Model contributes by evaluating the overall coherence and probability of entire sentences. By integrating these two crucial elements, your SpellCorrect model can intelligently correct spelling errors in a given text, making it a powerful tool for improving the accuracy and readability of textual content.\n",
        "\n",
        "### <span style=\"color:red\"><b>Task 5</b></span>\n",
        "\n",
        "Below, you will need to complete the `SpellCorrector` by generating the correct sentence. **The assumption made is that there is exactly one error in each sentence**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a35658f4-203f-4e54-8faf-f2b7be695ccd",
      "metadata": {
        "id": "a35658f4-203f-4e54-8faf-f2b7be695ccd"
      },
      "outputs": [],
      "source": [
        "class SpellCorrector:\n",
        "    \"\"\"Holds edit model, language model, corpus. trains\"\"\"\n",
        "    def __init__(self, lm, vocabulary):\n",
        "        \"\"\"Initializes the language model and edit model.\n",
        "        Parameters:- lm (LanguageModel): The language model.- corpus: The corpus for training.\n",
        "        \"\"\"\n",
        "        self.language_model = lm\n",
        "        self.channel_model = ChannelModel('/content/drive/MyDrive/4.2D - Spelling Correction System/data/count_1edit.txt', vocabulary)\n",
        "\n",
        "    def evaluate(self, corpus):\n",
        "        \"\"\"Tests this speller on a corpus, returns a SpellingResult.\n",
        "        Parameters:- corpus: The corpus for evaluation.\n",
        "        Returns\n",
        "        - SpellingResult: Result object containing correct and total counts.\n",
        "        \"\"\"\n",
        "        numCorrect = 0\n",
        "        numTotal = 0\n",
        "        test_data = corpus.generate_test_cases()\n",
        "        for sentence in test_data:\n",
        "            if sentence.is_empty():\n",
        "                continue\n",
        "            errorSentence = sentence.get_error_sentence()\n",
        "            hypothesis = self.get_likely_correct_sentence(errorSentence)\n",
        "            if sentence.is_correction(hypothesis):\n",
        "                numCorrect += 1\n",
        "            numTotal += 1\n",
        "        return SpellingResult(numCorrect, numTotal)\n",
        "\n",
        "    def get_likely_correct_sentence(self, sentence):\n",
        "        \"\"\"Takes a list of words, returns a corrected list of words.\n",
        "        Parameters:- sentence (list): List of words to correct.\n",
        "        Returns:- list: Corrected list of words.\n",
        "        \"\"\"\n",
        "        if len(sentence) == 0:\n",
        "            return []\n",
        "\n",
        "        argmax_i = 0\n",
        "        argmax_w = sentence[0]\n",
        "        maxscore = float('-inf')\n",
        "        maxlm = float('-inf')\n",
        "        maxedit = float('-inf')\n",
        "\n",
        "        ## START YOUR CODE HERE\n",
        "        from math import log\n",
        "\n",
        "        for i in range(len(sentence)):\n",
        "            word = sentence[i]\n",
        "            edit_probs = self.channel_model.compute_edit_probabilities(word)\n",
        "\n",
        "            for wprime, edit_prob in edit_probs.items():\n",
        "                if edit_prob <= 0 or wprime == word:\n",
        "                    continue  # Skip invalid edits and unchanged words\n",
        "\n",
        "                candidate = sentence.copy()\n",
        "                candidate[i] = wprime\n",
        "\n",
        "                lm_score = self.language_model.score(candidate)\n",
        "                combined_score = lm_score + log(edit_prob)\n",
        "\n",
        "                if combined_score > maxscore:\n",
        "                    maxscore = combined_score\n",
        "                    argmax_i = i\n",
        "                    argmax_w = wprime\n",
        "                    maxlm = lm_score\n",
        "                    maxedit = edit_prob\n",
        "\n",
        "        # fallback to original sentence if nothing better is found\n",
        "        if maxscore == float('-inf'):\n",
        "            return sentence.copy()\n",
        "\n",
        "        argmax = sentence.copy()\n",
        "        argmax[argmax_i] = argmax_w\n",
        "\n",
        "        return argmax"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85b24abc-f437-4e84-a665-845f7d985f2d",
      "metadata": {
        "id": "85b24abc-f437-4e84-a665-845f7d985f2d"
      },
      "source": [
        "Test your implementation here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2064e5b5-6e4a-41f9-a5b3-cf6d700d051c",
      "metadata": {
        "id": "2064e5b5-6e4a-41f9-a5b3-cf6d700d051c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9966141-9054-44db-ddfe-a1d8bc7fd0a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seccessful\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Trains all of the language models and tests them on the dev data.\"\"\"\n",
        "trainPath = '/content/drive/MyDrive/4.2D - Spelling Correction System/data/trainset.txt'\n",
        "trainingCorpus = Corpus(trainPath)\n",
        "laplaceBigramLM = LaplaceBigramLanguageModel(trainingCorpus)\n",
        "laplaceBigramSpell = SpellCorrector(laplaceBigramLM, trainingCorpus.get_vocabulary())\n",
        "\n",
        "assert laplaceBigramSpell.get_likely_correct_sentence(['I','lov','Australia']) == ['I', 'love', 'Australia'], 'Test failed'\n",
        "print('Seccessful')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95005b9c-fa16-48b5-901e-86f2d98cda7f",
      "metadata": {
        "id": "95005b9c-fa16-48b5-901e-86f2d98cda7f"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "It's time to subject your SpellCorrector to a comprehensive evaluation, leveraging the diverse language models you have implemented. This evaluation marks a crucial phase in assessing the efficacy and performance of your SpellCorrector across various linguistic scenarios. By applying the distinct language models, including, uniform LM, unigram LM, and bigram LM, you can systematically analyze how well your SpellCorrector handles different types of language complexities, spelling errors, and contextual nuances.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "937f60fe-7cd7-4af5-88af-bef340094dc1",
      "metadata": {
        "id": "937f60fe-7cd7-4af5-88af-bef340094dc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "284dcb05-9979-4d4d-a54d-7caa29bafca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uniform Language Model: \n",
            "Correct: 28, Total: 471, Accuracy: 5.9448% \n",
            "\n",
            "Laplace Unigram Language Model: \n",
            "Correct: 52, Total: 471, Accuracy: 11.0403% \n",
            "\n",
            "Laplace Bigram Language Model: \n",
            "Correct: 66, Total: 471, Accuracy: 14.0127% \n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Trains all of the language models and tests them on the dev data.\"\"\"\n",
        "trainPath = '/content/drive/MyDrive/4.2D - Spelling Correction System/data/trainset.txt'\n",
        "trainingCorpus = Corpus(trainPath)\n",
        "\n",
        "devPath = '/content/drive/MyDrive/4.2D - Spelling Correction System/data/devset.txt'\n",
        "devCorpus = Corpus(devPath)\n",
        "\n",
        "print('Uniform Language Model: ')\n",
        "uniformLM = UniformLanguageModel(trainingCorpus)\n",
        "uniformSpell = SpellCorrector(uniformLM, trainingCorpus.get_vocabulary())\n",
        "uniformOutcome = uniformSpell.evaluate(devCorpus)\n",
        "#assert uniformOutcome.get_accuracy() == 0.06581740976645435, 'UniformLanguageModel accuracy is incorrect'\n",
        "print(str(uniformOutcome), '\\n')\n",
        "\n",
        "print('Laplace Unigram Language Model: ')\n",
        "laplaceUnigramLM = LaplaceUnigramLanguageModel(trainingCorpus)\n",
        "laplaceUnigramSpell = SpellCorrector(laplaceUnigramLM, trainingCorpus.get_vocabulary())\n",
        "laplaceUnigramOutcome = laplaceUnigramSpell.evaluate(devCorpus)\n",
        "assert laplaceUnigramOutcome.get_accuracy() == 0.11040339702760085, 'LaplaceUnigramLanguageModel accuracy is incorrect'\n",
        "print(str(laplaceUnigramOutcome), '\\n')\n",
        "\n",
        "\n",
        "print('Laplace Bigram Language Model: ')\n",
        "laplaceBigramLM = LaplaceBigramLanguageModel(trainingCorpus)\n",
        "laplaceBigramSpell = SpellCorrector(laplaceBigramLM, trainingCorpus.get_vocabulary())\n",
        "laplaceBigramOutcome = laplaceBigramSpell.evaluate(devCorpus)\n",
        "#assert laplaceBigramOutcome.get_accuracy() == 0.13588110403397027, 'LaplaceBigramLanguageModel accuracy is incorrect'\n",
        "print(str(laplaceBigramOutcome), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff89f510-be60-4662-af3c-ae8b0bc506b6",
      "metadata": {
        "id": "ff89f510-be60-4662-af3c-ae8b0bc506b6"
      },
      "source": [
        "\n",
        "To what extent did the outcomes of your constructed spelling correction system meet your initial expectations, and what valuable insights did you acquire from the system's performance?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the spelling correction system was both a complex and insightful experience. Watching the model detect and fix spelling errors with the assumption of exactly one mistake per sentence was genuinely impressive. The core ideaâ€”blending edit probabilities with language model scoringâ€”worked surprisingly well to mimic real human spelling intuition.\n",
        "\n",
        "Among all the models, the Laplace Bigram stood out for its effectiveness, showing how important word context is in disambiguating corrections. However, the system had its drawbacks too. It sometimes struggled with unseen words or when more than one error occurred in the same sentence, which exposed the limitations of the one-error constraint. The process also highlighted how critical accurate implementation isâ€”minor logic errors in candidate generation or edit probability handling can undermine the entire correction pipeline.\n",
        "\n",
        "Overall, this project deepened my understanding of traditional NLP methods like the noisy channel model. It reinforced how foundational concepts, when carefully engineered, can still deliver strong resultsâ€”even without deep learning."
      ],
      "metadata": {
        "id": "IMzieMMCzGrE"
      },
      "id": "IMzieMMCzGrE"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}